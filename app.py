"""Streamlit web interface for Document QA Agent."""

import logging
import sys
from pathlib import Path

import streamlit as st

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))

from config import config
from embeddings import Embedder, TextSplitter, VectorStore
from parsers import BaseParser
from rag import RAGPipeline, YandexGPTClient, GigaChatClient

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)


# Page configuration
st.set_page_config(
    page_title="Document QA Agent",
    page_icon="üìö",
    layout="wide",
    initial_sidebar_state="expanded",
)


# Initialize session state
if "initialized" not in st.session_state:
    st.session_state.initialized = False
    st.session_state.embedder = None
    st.session_state.vector_store = None
    st.session_state.rag_pipeline = None
    st.session_state.chat_history = []


def initialize_system():
    """Initialize the QA system."""
    try:
        with st.spinner("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã..."):
            # Initialize embedder
            st.session_state.embedder = Embedder(
                model_name=config.embeddings.model,
                batch_size=config.embeddings.batch_size,
            )

            # Initialize vector store
            st.session_state.vector_store = VectorStore(
                persist_directory=config.vectordb.persist_directory,
                collection_name=config.vectordb.collection_name,
                distance_metric=config.vectordb.distance_metric,
            )

            # Initialize LLM client
            if config.llm.provider == "yandexgpt":
                llm_client = YandexGPTClient(
                    api_key=config.llm.api_key,
                    folder_id=config.llm.folder_id,
                    model=config.llm.model,
                    temperature=config.llm.temperature,
                    max_tokens=config.llm.max_tokens,
                )
            elif config.llm.provider == "gigachat":
                llm_client = GigaChatClient(
                    api_key=config.llm.api_key,
                    model=config.llm.model,
                    temperature=config.llm.temperature,
                    max_tokens=config.llm.max_tokens,
                )
            else:
                st.error(f"Unsupported LLM provider: {config.llm.provider}")
                return False

            # Initialize RAG pipeline
            st.session_state.rag_pipeline = RAGPipeline(
                llm_client=llm_client,
                embedder=st.session_state.embedder,
                vector_store=st.session_state.vector_store,
                top_k=config.retrieval.top_k,
                relevance_threshold=config.retrieval.relevance_threshold,
            )

            st.session_state.initialized = True
            return True

    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
        logger.error(f"Initialization error: {e}", exc_info=True)
        return False


def process_uploaded_files(uploaded_files):
    """Process and index uploaded documents."""
    if not uploaded_files:
        return

    with st.spinner("–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤..."):
        text_splitter = TextSplitter(
            chunk_size=config.embeddings.chunk_size,
            chunk_overlap=config.embeddings.chunk_overlap,
        )

        all_chunks = []

        for uploaded_file in uploaded_files:
            try:
                # Save uploaded file temporarily
                temp_path = Path(config.app.raw_docs_dir) / uploaded_file.name
                temp_path.parent.mkdir(parents=True, exist_ok=True)

                with open(temp_path, "wb") as f:
                    f.write(uploaded_file.getbuffer())

                # Parse document
                parser = BaseParser.get_parser(
                    temp_path,
                    extract_tables=config.parsing.extract_tables,
                )
                documents = parser.parse(temp_path)

                # Convert to dict format
                doc_dicts = [
                    {"text": doc.text, "metadata": doc.metadata}
                    for doc in documents
                ]

                # Split into chunks
                chunks = text_splitter.split_documents(doc_dicts)
                all_chunks.extend(chunks)

                st.success(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω: {uploaded_file.name}")

            except Exception as e:
                st.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {uploaded_file.name}: {e}")
                logger.error(f"Error processing {uploaded_file.name}: {e}", exc_info=True)

        if all_chunks:
            # Generate embeddings
            texts = [chunk["text"] for chunk in all_chunks]
            metadatas = [chunk["metadata"] for chunk in all_chunks]

            embeddings = st.session_state.embedder.embed_texts(
                texts,
                show_progress=False,
            )

            # Add to vector store
            st.session_state.vector_store.add_documents(
                texts=texts,
                embeddings=embeddings.tolist(),
                metadatas=metadatas,
            )

            st.success(f"–ü—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ {len(all_chunks)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∏–∑ {len(uploaded_files)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")


def main():
    """Main application."""
    st.title("üìö Document QA Agent")
    st.markdown("*–°–∏—Å—Ç–µ–º–∞ –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º*")

    # Sidebar
    with st.sidebar:
        st.header("‚öôÔ∏è –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ")

        # System status
        st.subheader("–°—Ç–∞—Ç—É—Å —Å–∏—Å—Ç–µ–º—ã")
        if st.session_state.initialized:
            st.success("‚úÖ –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞")
            if st.session_state.vector_store:
                doc_count = st.session_state.vector_store.get_collection_count()
                st.info(f"üìÑ –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –±–∞–∑–µ: {doc_count}")
        else:
            st.warning("‚è≥ –¢—Ä–µ–±—É–µ—Ç—Å—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è")
            if st.button("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å–∏—Å—Ç–µ–º—É"):
                if initialize_system():
                    st.rerun()

        st.divider()

        # Document upload
        st.subheader("üì§ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        uploaded_files = st.file_uploader(
            "–í—ã–±–µ—Ä–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã",
            type=["pdf", "docx", "doc", "md", "txt"],
            accept_multiple_files=True,
        )

        if uploaded_files and st.button("–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã"):
            if not st.session_state.initialized:
                st.error("–°–Ω–∞—á–∞–ª–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —Å–∏—Å—Ç–µ–º—É")
            else:
                process_uploaded_files(uploaded_files)

        st.divider()

        # Settings
        st.subheader("üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∏")
        st.info(f"""
        **LLM:** {config.llm.provider}
        **–ú–æ–¥–µ–ª—å:** {config.llm.model}
        **Top-K:** {config.retrieval.top_k}
        **–ü–æ—Ä–æ–≥ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏:** {config.retrieval.relevance_threshold}
        """)

        if st.button("–û—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é"):
            st.session_state.chat_history = []
            st.rerun()

    # Main area
    if not st.session_state.initialized:
        st.info("üëà –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —Å–∏—Å—Ç–µ–º—É —á–µ—Ä–µ–∑ –±–æ–∫–æ–≤—É—é –ø–∞–Ω–µ–ª—å")
        return

    # Chat interface
    st.subheader("üí¨ –ó–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å")

    # Display chat history
    for message in st.session_state.chat_history:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

            # Show sources for assistant messages
            if message["role"] == "assistant" and "sources" in message:
                with st.expander(f"üìö –ò—Å—Ç–æ—á–Ω–∏–∫–∏ ({message.get('num_sources', 0)})"):
                    for i, source in enumerate(message["sources"], 1):
                        metadata = source.get("metadata", {})
                        st.markdown(f"""
                        **–ò—Å—Ç–æ—á–Ω–∏–∫ {i}** (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {source.get('similarity', 0):.2%})
                        *–§–∞–π–ª:* {metadata.get('file_name', 'Unknown')}
                        *–°—Ç—Ä–∞–Ω–∏—Ü–∞:* {metadata.get('page_number', 'N/A')}

                        {source.get('text', '')}
                        """)
                        st.divider()

    # Chat input
    if query := st.chat_input("–í–≤–µ–¥–∏—Ç–µ –≤–∞—à –≤–æ–ø—Ä–æ—Å..."):
        # Add user message
        st.session_state.chat_history.append({"role": "user", "content": query})

        with st.chat_message("user"):
            st.markdown(query)

        # Generate response
        with st.chat_message("assistant"):
            with st.spinner("–ü–æ–∏—Å–∫ –æ—Ç–≤–µ—Ç–∞..."):
                result = st.session_state.rag_pipeline.answer_question(query)

                st.markdown(result["answer"])

                # Show sources
                if result.get("sources"):
                    with st.expander(f"üìö –ò—Å—Ç–æ—á–Ω–∏–∫–∏ ({result['num_sources']})"):
                        for i, source in enumerate(result["sources"], 1):
                            metadata = source.get("metadata", {})
                            st.markdown(f"""
                            **–ò—Å—Ç–æ—á–Ω–∏–∫ {i}** (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {source.get('similarity', 0):.2%})
                            *–§–∞–π–ª:* {metadata.get('file_name', 'Unknown')}
                            *–°—Ç—Ä–∞–Ω–∏—Ü–∞:* {metadata.get('page_number', 'N/A')}

                            {source.get('text', '')}
                            """)
                            st.divider()

        # Add assistant message
        st.session_state.chat_history.append(
            {
                "role": "assistant",
                "content": result["answer"],
                "sources": result.get("sources", []),
                "num_sources": result.get("num_sources", 0),
            }
        )


if __name__ == "__main__":
    main()
